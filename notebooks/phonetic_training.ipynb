{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the Phonetic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.pairing.dataset.phonetic_pair_dataset import PhoneticPairDataset\n",
    "from src.pairing.dataset.phonetic_triplet_dataset import PhoneticTripletDataset\n",
    "from src.pairing.model.phonetic_siamese import PhoneticSiamese\n",
    "from src.pairing.training.config import CONFIG, LossType\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from jina import DocumentArray, Document\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import torch\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset() -> Dataset:\n",
    "    \"\"\"Returns a Dataset object given loss type.\"\"\"\n",
    "    if CONFIG.loss_type == LossType.Pair:\n",
    "        dataset = PhoneticPairDataset(\n",
    "            best_pairs_path=CONFIG.best_pairs_dataset, worst_pairs_path=CONFIG.worst_pairs_dataset\n",
    "        )\n",
    "    elif CONFIG.loss_type == LossType.Triplet:\n",
    "        dataset = PhoneticTripletDataset(\n",
    "            best_pairs_path=CONFIG.best_pairs_dataset, worst_pairs_path=CONFIG.worst_pairs_dataset\n",
    "        )\n",
    "    elif CONFIG.loss_type == LossType.Mixed:\n",
    "        dataset = PhoneticTripletDataset(\n",
    "            best_pairs_path=CONFIG.best_pairs_dataset, worst_pairs_path=CONFIG.worst_pairs_dataset\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unknown loss type given: {CONFIG.loss_type}')\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third = len(dataset)//3\n",
    "# train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "#     dataset, [third, third, len(dataset)-2*third]\n",
    "# )\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "    dataset, [len(dataset)-600, 300, 300]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instanciate(kwargs):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_set, batch_size=kwargs[\"batch_size\"], shuffle=True, num_workers=4\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        val_set, batch_size=kwargs[\"batch_size\"], num_workers=4\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_set, batch_size=kwargs[\"batch_size\"], num_workers=4\n",
    "    )\n",
    "    model = PhoneticSiamese(\n",
    "        embedding_dim=kwargs[\"embedding_dim\"],\n",
    "        dim_feedforward=kwargs[\"dim_feedforward\"],\n",
    "        nhead=kwargs[\"nhead\"],\n",
    "        dropout=kwargs[\"dropout\"],\n",
    "        loss_type=CONFIG.loss_type,\n",
    "        batch_size=kwargs[\"batch_size\"],\n",
    "        weight_decay=kwargs[\"weight_decay\"],\n",
    "        lr=kwargs[\"lr\"],\n",
    "        margin=kwargs[\"margin\"],\n",
    "        lambda_triplet=kwargs[\"lambda_triplet\"],\n",
    "        lambda_pos=kwargs[\"lambda_pos\"],\n",
    "        lambda_neg=kwargs[\"lambda_neg\"],\n",
    "    )\n",
    "    return {\n",
    "        \"train_dataloader\": train_dataloader,\n",
    "        \"validation_dataloader\": validation_dataloader,\n",
    "        \"test_dataloader\": test_dataloader,\n",
    "        \"model\": model,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        dropout,\n",
    "        lr,\n",
    "        weight_decay,\n",
    "        dim_feedforward,\n",
    "        batch_size,\n",
    "        nhead,\n",
    "        embedding_dim,\n",
    "        margin\n",
    "):\n",
    "    mlf_logger = MLFlowLogger(\n",
    "        experiment_name=CONFIG.experiment_name, tracking_uri=CONFIG.log_folder\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        max_epochs=CONFIG.max_epochs,\n",
    "        logger=mlf_logger,\n",
    "        # callbacks=[EarlyStopping(monitor=\"validation_loss\", mode=\"min\")],\n",
    "        accelerator=\"gpu\", \n",
    "        devices=1\n",
    "    )\n",
    "    instance = instanciate(\n",
    "        {\n",
    "            \"dropout\": dropout,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"dim_feedforward\": dim_feedforward,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"nhead\": nhead,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"margin\": margin,\n",
    "            \"model\": \"phonetic_siamese\",\n",
    "            \"lambda_triplet\": 0.5,\n",
    "            \"lambda_pos\": 0.5,\n",
    "            \"lambda_neg\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    mlflow.pytorch.autolog()\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        model = fit_model(\n",
    "            instance[\"model\"],\n",
    "            instance[\"train_dataloader\"],\n",
    "            instance[\"validation_dataloader\"],\n",
    "            trainer,\n",
    "        )\n",
    "\n",
    "        test_loss = test_model(model, instance[\"test_dataloader\"], trainer)[0][\"test_loss\"]\n",
    "\n",
    "        torch.save(model.state_dict(), \"model_dict\")\n",
    "        torch.save(model.state_dict(), \"../src/pairing/model/model_dict\")\n",
    "        mlflow.log_artifact(\"model_dict\", \"model_dict\")\n",
    "\n",
    "    return model, test_loss\n",
    "\n",
    "def fit_model(model, train_dataloader, validation_dataloader, trainer):\n",
    "    trainer.fit(model, train_dataloader, validation_dataloader)\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_dataloader, trainer):\n",
    "    return trainer.test(model, test_dataloader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, test_loss = train(\n",
    "    dropout=0.2,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-3,\n",
    "    dim_feedforward=16,\n",
    "    batch_size=16,\n",
    "    nhead=0,\n",
    "    embedding_dim=16,\n",
    "    margin=0.2\n",
    ")\n",
    "\n",
    "print(f'Final test loss: {test_loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:0\")\n",
    "\n",
    "def load_documents() -> DocumentArray:\n",
    "    dataframe = pd.read_csv(Path(\"../src/pairing/dataset/pairing/english.csv\"))\n",
    "    words = dataframe[['word', 'ipa']].astype(str)  \n",
    "\n",
    "    local_da = DocumentArray([Document(text=w['word'], ipa=w['ipa']) for _, w in words.iterrows()])\n",
    "    def embed(da: DocumentArray) -> DocumentArray:\n",
    "            x = da[:,'tags__ipa']\n",
    "            da.embeddings = model.encode(x).detach().cpu() \n",
    "            return da\n",
    "    local_da.apply_batch(embed, batch_size=32)\n",
    "\n",
    "    with DocumentArray() as da:\n",
    "        da += local_da\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(da, n_limit=10):\n",
    "    sample_data = dataset.best_pairs.sample(100)\n",
    "\n",
    "    queries = sample_data['ipa_a']\n",
    "    query_text = sample_data['word_a']\n",
    "    targets = sample_data['word_b']\n",
    "\n",
    "    # 1. Generate all embeddings for pairs.\n",
    "    query_embeddings = model.encode(queries).detach().cpu() \n",
    "    query_docs = DocumentArray([Document(text=text, ipa=ipa, embedding=embedding) for ipa, text, embedding in zip(queries, query_text, query_embeddings)])\n",
    "\n",
    "    # 2. For each pair: query the da\n",
    "    [\n",
    "        doc.match(da, metric='euclidean', limit=n_limit) for doc in query_docs\n",
    "    ]\n",
    "\n",
    "    matches = [\n",
    "        doc.matches[:,'text'] for doc in query_docs\n",
    "    ]\n",
    "\n",
    "    # 3. For each pair: check if targetted word is amongst results\n",
    "    scores = [\n",
    "        t in match for t, match in zip(targets, matches)\n",
    "    ]\n",
    "    return scores, query_text, targets, matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = load_documents()\n",
    "\n",
    "scores, queries, targets, matches = evaluate(da, n_limit=50)\n",
    "print(f\"Percentage of the original pair that have been retrieved: {sum(scores) / len(scores)*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, t, m in zip(queries, targets, matches):\n",
    "    print(q, t, m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model.eval().to(device)\n",
    "\n",
    "for i in np.random.randint(0, len(test_set), 10):\n",
    "    sample = test_set[i]\n",
    "    anchor_match = sample['anchor_phonetic']\n",
    "    positive_match = sample['similar_phonetic']\n",
    "    negative_match = sample['distant_phonetic']\n",
    "\n",
    "    anchor_embedding = model.encode([anchor_match])\n",
    "    positive_embedding = model.encode([positive_match])\n",
    "    negative_embedding = model.encode([negative_match])\n",
    "\n",
    "    loss = model.triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "\n",
    "    positive_dist = torch.cdist(anchor_embedding, positive_embedding, p=2)\n",
    "    negative_dist = torch.cdist(anchor_embedding, negative_embedding, p=2)\n",
    "\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Positive: {positive_dist}\")\n",
    "    print(f\"Negative: {negative_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eng_to_ipa import convert\n",
    "words = ['dog', 'parade', 'cascade', \"palace\", \"table\"]\n",
    "ipas = [convert(x) for x in words]\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model.eval().to(device)\n",
    "embdedings = model.encode(ipas)\n",
    "\n",
    "a = torch.cdist(embdedings[1].view(1, -1), embdedings[0].view(1, -1), p=2)\n",
    "b = torch.cdist(embdedings[1].view(1, -1), embdedings[2].view(1, -1), p=2)\n",
    "c = torch.cdist(embdedings[1].view(1, -1), embdedings[3].view(1, -1), p=2)\n",
    "d = torch.cdist(embdedings[1].view(1, -1), embdedings[4].view(1, -1), p=2)\n",
    "\n",
    "a, b, c, d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bce2850d3edd15649e80217fbd55dcd373df4e90b8ec4a1ddad38e9f78a7b499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

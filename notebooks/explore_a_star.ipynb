{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Creation\n",
    "\n",
    "Generate a graph sampled from Wikipedia using BFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_node = \"Koala\" # Initial node to start BFS from.\n",
    "hop_nb = 100 # Number of pages allowed to be visited.\n",
    "graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [wiki_wiki.page(starting_node)]\n",
    "\n",
    "forbidden_protocols = [\"Category\", \"Template\", \"Wikipedia\", \"User\", \"Help\", \"Talk\", \"Portal\", \"File\", \"Module\"]\n",
    "\n",
    "node2page = {queue[0].title: queue[0]}\n",
    "\n",
    "for hop in tqdm(range(hop_nb)):\n",
    "    if len(queue) > 0:\n",
    "        page = queue.pop()\n",
    "        node2page[page.title] = page\n",
    "        for name, neighbor_page in page.links.items():\n",
    "            if all([not name.startswith(x) for x in forbidden_protocols]):\n",
    "                if name not in graph.nodes and neighbor_page not in queue: # If not yet visited & not in queue already\n",
    "                    queue.append(neighbor_page)\n",
    "                graph.add_edge(page.title, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2page.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset of pairs and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform([\"bonjour à tous.\", \"Au revoir à tous.\", \"J'ai vu un chien\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tfidf[0]*tfidf[2].T).toarray()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pair_nb = 100) -> pd.DataFrame:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    samples = np.random.choice(list(node2page.keys()), pair_nb*2).tolist()\n",
    "    summaries = [node2page[x].summary for x in samples]\n",
    "    categories = [node2page[x].categories for x in tqdm(samples)]\n",
    "    docs = [x for x in nlp.pipe(samples)]\n",
    "    pairs = np.array(range(len(samples))).reshape(2, -1) \n",
    "    tfidf = TfidfVectorizer().fit_transform(summaries)\n",
    "    rows = []\n",
    "\n",
    "    for src, tgt in tqdm(zip(*pairs)):\n",
    "        length = nx.shortest_path_length(graph, samples[src], samples[tgt])\n",
    "        doc_a = docs[src]\n",
    "        doc_b = docs[tgt]\n",
    "        degree_a = graph.degree[samples[src]]\n",
    "        degree_b = graph.degree[samples[tgt]]\n",
    "        similarity = doc_a.similarity(doc_b)\n",
    "        common_categories = len(set(categories[src]).intersection(categories[tgt]))\n",
    "        all_categories = len(set(categories[src]).union(categories[tgt])) + 1\n",
    "        rows.append({\n",
    "            \"length\": length, \n",
    "            \"src\": samples[src], \n",
    "            \"tgt\": samples[tgt], \n",
    "            \"distance\": 1 / (1e-1 + similarity), \n",
    "            \"similarity\": similarity, \n",
    "            \"common_cat\": common_categories, \n",
    "            \"percent_cat\": common_categories / all_categories, \n",
    "            \"degree_sum\": degree_a + degree_b, \n",
    "            \"degree_diff\": abs(degree_a - degree_b),\n",
    "            \"tfidf\": (tfidf[src]*tfidf[tgt].T).toarray()[0][0]\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = create_dataset(5000)\n",
    "test_set = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.sort_values('length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[['length', \"common_cat\", \"degree_sum\", \"degree_diff\", \"similarity\", \"distance\", \"percent_cat\", \"tfidf\"]].corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array(train_set['embedding_diff'].to_list())\n",
    "X_train = train_set[[\"common_cat\", \"degree_sum\", \"degree_diff\", \"similarity\", \"distance\", \"percent_cat\", \"tfidf\"]]\n",
    "y_train = train_set['length']\n",
    "\n",
    "# X_test = np.array(test_set['embedding_diff'].to_list())\n",
    "X_test = test_set[[\"common_cat\", \"degree_sum\", \"degree_diff\", \"similarity\", \"distance\", \"percent_cat\", \"tfidf\"]]\n",
    "y_test = test_set['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.cuda.is_available()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bce2850d3edd15649e80217fbd55dcd373df4e90b8ec4a1ddad38e9f78a7b499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import networkx as nx\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Creation\n",
    "\n",
    "Generate a graph sampled from Wikipedia using BFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_node = \"Koala\" # Initial node to start BFS from.\n",
    "hop_nb = 10 # Number of pages allowed to be visited.\n",
    "graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [wiki_wiki.page(starting_node)]\n",
    "\n",
    "forbidden_protocols = [\"Category\", \"Template\", \"Wikipedia\", \"User\", \"Help\", \"Talk\", \"Portal\"]\n",
    "\n",
    "node2page = {queue[0].title: queue[0]}\n",
    "\n",
    "for hop in range(hop_nb):\n",
    "    if len(queue) > 0:\n",
    "        page = queue.pop()\n",
    "        for name, neighbor_page in page.links.items():\n",
    "            if all([not name.startswith(x) for x in forbidden_protocols]):\n",
    "                if name not in graph.nodes and neighbor_page not in queue: # If not yet visited & not in queue already\n",
    "                    queue.append(neighbor_page)\n",
    "                graph.add_edge(page.title, name)\n",
    "                node2page[name] = neighbor_page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset of pairs and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pair_nb = 100) -> pd.DataFrame:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    samples = np.random.choice(list(node2page.keys()), pair_nb*2).tolist()\n",
    "    summaries = [node2page[x].summary for x in samples]\n",
    "    docs = [x for x in nlp.pipe(summaries)]\n",
    "    pairs = np.array(range(len(samples))).reshape(2, -1) \n",
    "    rows = []\n",
    "\n",
    "    for src, tgt in zip(*pairs):\n",
    "        length = nx.shortest_path_length(graph, samples[src], samples[tgt])\n",
    "        doc_a = docs[src]\n",
    "        doc_b = docs[tgt]\n",
    "        distance = 1 / (1e-1 + doc_a.similarity(doc_b))\n",
    "        distance = -log(doc_a.similarity(doc_b) + 1e-3)\n",
    "        rows.append({\"length\": length, \"src\": samples[src], \"tgt\": samples[tgt], \"distance\": distance})\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_30184\\4126465098.py:13: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  distance = 1 / (1e-1 + doc_a.similarity(doc_b))\n",
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_30184\\4126465098.py:14: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  distance = -log(doc_a.similarity(doc_b) + 1e-3)\n",
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_30184\\4126465098.py:13: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  distance = 1 / (1e-1 + doc_a.similarity(doc_b))\n",
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_30184\\4126465098.py:14: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  distance = -log(doc_a.similarity(doc_b) + 1e-3)\n"
     ]
    }
   ],
   "source": [
    "# train_set = create_dataset(10)\n",
    "test_set = create_dataset(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Noozles</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>0.041987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2</td>\n",
       "      <td>Pathogen</td>\n",
       "      <td>Printmaking</td>\n",
       "      <td>0.061552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>Diprotodontoidea</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.068033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>4</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Occupational Safety and Health Act</td>\n",
       "      <td>0.068669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>Compassion</td>\n",
       "      <td>Sigmund Freud</td>\n",
       "      <td>0.083609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>3</td>\n",
       "      <td>Enterprise communities</td>\n",
       "      <td>Free-range parenting</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Vocational Rehabilitation Act</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4</td>\n",
       "      <td>Nimbavombatus</td>\n",
       "      <td>Youth leadership</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4</td>\n",
       "      <td>Kangaroo Island</td>\n",
       "      <td>Budget Reconciliation Act</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>Revenue Adjustment Act</td>\n",
       "      <td>Zygote</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     length                     src                                 tgt  \\\n",
       "97        5                 Noozles                             Seattle   \n",
       "77        2                Pathogen                         Printmaking   \n",
       "85        5        Diprotodontoidea                       United States   \n",
       "177       4           San Francisco  Occupational Safety and Health Act   \n",
       "22        2              Compassion                       Sigmund Freud   \n",
       "..      ...                     ...                                 ...   \n",
       "165       3  Enterprise communities                Free-range parenting   \n",
       "129       4                  Mexico       Vocational Rehabilitation Act   \n",
       "118       4           Nimbavombatus                    Youth leadership   \n",
       "83        4         Kangaroo Island           Budget Reconciliation Act   \n",
       "42        4  Revenue Adjustment Act                              Zygote   \n",
       "\n",
       "     distance  \n",
       "97   0.041987  \n",
       "77   0.061552  \n",
       "85   0.068033  \n",
       "177  0.068669  \n",
       "22   0.083609  \n",
       "..        ...  \n",
       "165  6.907755  \n",
       "129  6.907755  \n",
       "118  6.907755  \n",
       "83   6.907755  \n",
       "42   6.907755  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.sort_values('distance')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_set['embedding_diff'].to_list())\n",
    "X_train = train_set[['distance']]\n",
    "y_train = train_set['length']\n",
    "\n",
    "X_test = np.array(test_set['embedding_diff'].to_list())\n",
    "X_test = test_set[['distance']]\n",
    "y_test = test_set['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.016786290624264844"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bce2850d3edd15649e80217fbd55dcd373df4e90b8ec4a1ddad38e9f78a7b499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the Phonetic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\anaconda3\\envs\\geometric\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.pairing.dataset.phonetic_pair_dataset import PhoneticPairDataset\n",
    "from src.pairing.dataset.phonetic_triplet_dataset import PhoneticTripletDataset\n",
    "from src.pairing.model.phonetic_siamese import PhoneticSiamese\n",
    "from src.pairing.training.config import CONFIG, LossType\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset() -> Dataset:\n",
    "    \"\"\"Returns a Dataset object given loss type.\"\"\"\n",
    "    if CONFIG.loss_type == LossType.Pair:\n",
    "        dataset = PhoneticPairDataset(\n",
    "            best_pairs_path=CONFIG.best_pairs_dataset, worst_pairs_path=CONFIG.worst_pairs_dataset\n",
    "        )\n",
    "    elif CONFIG.loss_type == LossType.Triplet:\n",
    "        dataset = PhoneticTripletDataset(\n",
    "            best_pairs_path=CONFIG.best_pairs_dataset, worst_pairs_path=CONFIG.worst_pairs_dataset\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unknown loss type given: {CONFIG.loss_type}')\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "third = len(dataset)//3\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "    dataset, [third, third, len(dataset)-2*third]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instanciate(kwargs):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_set, batch_size=kwargs[\"batch_size\"], shuffle=True, num_workers=4\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        val_set, batch_size=kwargs[\"batch_size\"], num_workers=4\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_set, batch_size=kwargs[\"batch_size\"], num_workers=4\n",
    "    )\n",
    "    model = PhoneticSiamese(\n",
    "        embedding_dim=kwargs[\"embedding_dim\"],\n",
    "        dim_feedforward=kwargs[\"dim_feedforward\"],\n",
    "        nhead=kwargs[\"nhead\"],\n",
    "        dropout=kwargs[\"dropout\"],\n",
    "        loss_type=CONFIG.loss_type,\n",
    "        batch_size=kwargs[\"batch_size\"],\n",
    "        weight_decay=kwargs[\"weight_decay\"],\n",
    "        lr=kwargs[\"lr\"],\n",
    "        margin=kwargs[\"margin\"]\n",
    "    )\n",
    "    return {\n",
    "        \"train_dataloader\": train_dataloader,\n",
    "        \"validation_dataloader\": validation_dataloader,\n",
    "        \"test_dataloader\": test_dataloader,\n",
    "        \"model\": model,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        dropout,\n",
    "        lr,\n",
    "        weight_decay,\n",
    "        dim_feedforward,\n",
    "        batch_size,\n",
    "        nhead,\n",
    "        embedding_dim,\n",
    "        margin\n",
    "):\n",
    "    mlf_logger = MLFlowLogger(\n",
    "        experiment_name=CONFIG.experiment_name, tracking_uri=CONFIG.log_folder\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        max_epochs=CONFIG.max_epochs,\n",
    "        logger=mlf_logger,\n",
    "        callbacks=[EarlyStopping(monitor=\"validation_loss\", mode=\"min\")],\n",
    "        accelerator=\"gpu\", devices=1\n",
    "    )\n",
    "    instance = instanciate(\n",
    "        {\n",
    "            \"dropout\": dropout,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"dim_feedforward\": dim_feedforward,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"nhead\": nhead,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"margin\": margin,\n",
    "            \"model\": \"phonetic_siamese\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    mlflow.pytorch.autolog()\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        model = fit_model(\n",
    "            instance[\"model\"],\n",
    "            instance[\"train_dataloader\"],\n",
    "            instance[\"validation_dataloader\"],\n",
    "            trainer,\n",
    "        )\n",
    "\n",
    "        test_loss = test_model(model, instance[\"test_dataloader\"], trainer)[0][\"test_loss\"]\n",
    "\n",
    "        torch.save(model.state_dict(), \"model_dict\")\n",
    "        mlflow.log_artifact(\"model_dict\", \"model_dict\")\n",
    "\n",
    "    return model, test_loss\n",
    "\n",
    "def fit_model(model, train_dataloader, validation_dataloader, trainer):\n",
    "    trainer.fit(model, train_dataloader, validation_dataloader)\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_dataloader, trainer):\n",
    "    return trainer.test(model, test_dataloader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type                    | Params\n",
      "---------------------------------------------------------------\n",
      "0 | embedding          | Embedding               | 9.4 K \n",
      "1 | encoder            | TransformerEncoderLayer | 1.7 K \n",
      "2 | cos                | CosineSimilarity        | 0     \n",
      "3 | p_enc_1d_model     | PositionalEncoding1D    | 0     \n",
      "4 | p_enc_1d_model_sum | Summer                  | 0     \n",
      "5 | triplet_loss       | TripletMarginLoss       | 0     \n",
      "---------------------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 110/110 [00:14<00:00,  7.35it/s, loss=0, v_num=0b1c, validation_loss=0.014, training_loss=0.0116]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/23 09:48:39 WARNING mlflow.utils.requirements_utils: Found torch version (1.13.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torch==1.13.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/02/23 09:48:46 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2022-11-11; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'finding-mnemo'}\n",
      "2023/02/23 09:48:46 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.14.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torchvision==0.14.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/02/23 09:48:46 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\simon\\anaconda3\\envs\\geometric\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 55/55 [00:00<00:00, 166.67it/s]\n",
      "Final test loss: 0.004024611786007881\n"
     ]
    }
   ],
   "source": [
    "model, test_loss = train(\n",
    "    dropout=0.2,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-3,\n",
    "    dim_feedforward=16,\n",
    "    batch_size=16,\n",
    "    nhead=2,\n",
    "    embedding_dim=16,\n",
    "    margin=0.2\n",
    ")\n",
    "\n",
    "print(f'Final test loss: {test_loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0\n",
      "Positive: tensor([[11.8748]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[21.9532]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.0\n",
      "Positive: tensor([[6.4286]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[9.6256]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.0\n",
      "Positive: tensor([[8.8924]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[10.4454]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.4046163558959961\n",
      "Positive: tensor([[8.4165]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[8.2118]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.0\n",
      "Positive: tensor([[8.1106]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[9.7890]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.9210586547851562\n",
      "Positive: tensor([[8.8377]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[8.1167]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.0\n",
      "Positive: tensor([[5.1002]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[6.3311]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.0\n",
      "Positive: tensor([[11.7434]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[17.1791]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.0\n",
      "Positive: tensor([[8.0059]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[16.3098]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Loss: 0.0\n",
      "Positive: tensor([[8.0089]], device='cuda:0', grad_fn=<CdistBackward0>)\n",
      "Negative: tensor([[8.4586]], device='cuda:0', grad_fn=<CdistBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.eval().to(device)\n",
    "\n",
    "for i in np.random.randint(0, len(test_set), 10):\n",
    "    sample = test_set[i]\n",
    "    anchor_match = sample['anchor_phonetic']\n",
    "    positive_match = sample['similar_phonetic']\n",
    "    negative_match = sample['distant_phonetic']\n",
    "\n",
    "    anchor_embedding = model.encode([anchor_match])\n",
    "    positive_embedding = model.encode([positive_match])\n",
    "    negative_embedding = model.encode([negative_match])\n",
    "\n",
    "    loss = model.triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "\n",
    "    positive_dist = torch.cdist(anchor_embedding, positive_embedding, p=2)\n",
    "    negative_dist = torch.cdist(anchor_embedding, negative_embedding, p=2)\n",
    "\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Positive: {positive_dist}\")\n",
    "    print(f\"Negative: {negative_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8.9612]], device='cuda:0', grad_fn=<CdistBackward0>),\n",
       " tensor([[13.3801]], device='cuda:0', grad_fn=<CdistBackward0>),\n",
       " tensor([[5.5410]], device='cuda:0', grad_fn=<CdistBackward0>),\n",
       " tensor([[7.6848]], device='cuda:0', grad_fn=<CdistBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eng_to_ipa import convert\n",
    "words = ['dog', 'parade', 'cascade', \"palace\", \"table\"]\n",
    "ipas = [convert(x) for x in words]\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model.eval().to(device)\n",
    "embdedings = model.encode(ipas)\n",
    "\n",
    "a = torch.cdist(embdedings[1].view(1, -1), embdedings[0].view(1, -1), p=2)\n",
    "b = torch.cdist(embdedings[1].view(1, -1), embdedings[2].view(1, -1), p=2)\n",
    "c = torch.cdist(embdedings[1].view(1, -1), embdedings[3].view(1, -1), p=2)\n",
    "d = torch.cdist(embdedings[1].view(1, -1), embdedings[4].view(1, -1), p=2)\n",
    "\n",
    "a, b, c, d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bce2850d3edd15649e80217fbd55dcd373df4e90b8ec4a1ddad38e9f78a7b499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
